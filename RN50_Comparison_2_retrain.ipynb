{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "T7hHO8VuDr2b",
        "aI8hYDpInBnt"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b4250d5b81834ad083b0cd7c2a904a8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_32d0231b4c9d4c81ae9995ac2b272f1f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2399a30170d74966b01c3353a2f290df",
              "IPY_MODEL_e29ba468730f4497bb5e2f1ca9b60091",
              "IPY_MODEL_be1194b6390c4c87a653cedd6a7ad559"
            ]
          }
        },
        "32d0231b4c9d4c81ae9995ac2b272f1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2399a30170d74966b01c3353a2f290df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1bd85d17e3be46cba3db6e5553e13c82",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f2571936709442669929a7a57a9f67ce"
          }
        },
        "e29ba468730f4497bb5e2f1ca9b60091": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ce6ef29f29be454291f7812e306eb468",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102530333,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102530333,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_173eebea58c44849b26f357e65a25daa"
          }
        },
        "be1194b6390c4c87a653cedd6a7ad559": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_781bd2455e4248b4b5bc80ac5bd53e03",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:02&lt;00:00, 39.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c5ce94dc489b4ac6849c71249a1c249d"
          }
        },
        "1bd85d17e3be46cba3db6e5553e13c82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f2571936709442669929a7a57a9f67ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ce6ef29f29be454291f7812e306eb468": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "173eebea58c44849b26f357e65a25daa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "781bd2455e4248b4b5bc80ac5bd53e03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c5ce94dc489b4ac6849c71249a1c249d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#2.0"
      ],
      "metadata": {
        "id": "T7hHO8VuDr2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[New paper](https://www.ripublication.com/ijaer19/ijaerv14n8_33.pdf)"
      ],
      "metadata": {
        "id": "b0rStS81Dvhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import argparse\n",
        "from myconfig import configt\n",
        "from keras import optimizers\n",
        "from keras.utils import np_utils\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "keras.backend.tensorflow_backend.set_session(tf.Session(config=config))\n",
        "\n",
        "#conv_base = ResNet50(weights='imagenet', include_top=False, input_shape=(230, 230, 3))\n",
        "#from keras.applications.xception import Xception\n",
        "#conv_base = Xception(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
        "\n",
        "\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import numpy as np\n",
        "from imutils import paths\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "from keras.layers import Average\n",
        "from keras.layers import merge\n",
        "\n",
        "from keras import applications\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "from keras.models import Model, Input\n",
        "\n",
        "import argparse\n",
        "\n",
        "from keras import optimizers\n",
        "import seaborn as sns\n",
        "#from keras.engine.topology import Input\n",
        "\n",
        "# construct the argument parser and parse the arguments\n",
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n",
        "\thelp=\"path to output loss/accuracy plot\")\n",
        "args = vars(ap.parse_args())\n",
        "\n",
        "# initialize our number of epochs, initial learning rate, and batch\n",
        "# size\n",
        "\n",
        "\n",
        "chanDim=-1\n",
        "\n",
        "def resnet500(model_input):\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=model_input)\n",
        "    last = base_model.output\n",
        "    x = layers.Flatten()(last)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    #preds = Dense(8, activation='softmax')(x)\n",
        "    #model = Model(base_model.input, preds)\n",
        "    model = Model(base_model.input, x)\n",
        "\n",
        "    base_model.trainable = True\n",
        "    set_trainable = False\n",
        "    for layer in base_model.layers:\n",
        "        if layer.name == 'add_30': # 从这一层开始往后均可训练\n",
        "            set_trainable = True\n",
        "            if set_trainable:\n",
        "                layer.trainable = True\n",
        "            else:\n",
        "                layer.trainable = False\n",
        "\n",
        "    return model\n",
        "\n",
        "def vgg166(model_input):\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_tensor=model_input)\n",
        "    last = base_model.output\n",
        "    x = layers.Flatten()(last)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    #preds = Dense(8, activation='softmax')(x)\n",
        "    model = Model(base_model.input, x)\n",
        "\n",
        "    base_model.trainable = True\n",
        "    set_trainable = False\n",
        "    for layer in base_model.layers:\n",
        "        if layer.name == 'block5_conv1': # 从这一层开始往后均可训练\n",
        "            set_trainable = True\n",
        "            if set_trainable:\n",
        "                layer.trainable = True\n",
        "            else:\n",
        "                layer.trainable = False\n",
        "\n",
        "    return model\n",
        "\n",
        "model_input = Input(shape=(230, 230, 3))\n",
        "\n",
        "\n",
        "model1 = resnet500(model_input)\n",
        "model2 = vgg166(model_input)\n",
        "\n",
        "ensembled_models = [model1,model2]\n",
        "def ensemble(models,model_input):\n",
        "    outputs = [model.outputs[0] for model in models]\n",
        "    #modelo1 = model1.outputs[0]\n",
        "    #modelo2 = model2.outputs[0]\n",
        "    modelo1 = outputs[0]\n",
        "    modelo2 = outputs[1]\n",
        "    y = layers.concatenate([modelo1, modelo2], axis=-1)\n",
        "    preds = Dense(8, activation='softmax')(y)\n",
        "    model = Model(model_input,preds,name='ensemble')\n",
        "    return model\n",
        "\n",
        "\n",
        "ensemble_model = ensemble(ensembled_models,model_input)\n",
        "\n",
        "\n",
        "from keras.utils import plot_model\n",
        "plot_model(ensemble_model, show_shapes=True, to_file='bcdnet_comodel.png')\n",
        "#added = keras.layers.add([input1, input2])\n",
        "\n",
        "\n",
        "# construct the argument parser and parse the arguments\n",
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n",
        "\thelp=\"path to output loss/accuracy plot\")\n",
        "args = vars(ap.parse_args())\n",
        "\n",
        "# initialize our number of epochs, initial learning rate, and batch\n",
        "# size\n",
        "NUM_EPOCHS = 38\n",
        "INIT_LR = 1e-2\n",
        "BS = 16\n",
        "\n",
        "# determine the total number of image paths in training, validation,\n",
        "# and testing directories\n",
        "trainPaths = list(paths.list_images(configt.TRAIN_PATH))\n",
        "totalTrain = len(trainPaths)\n",
        "totalVal = len(list(paths.list_images(configt.VAL_PATH)))\n",
        "totalTest = len(list(paths.list_images(configt.TEST_PATH)))\n",
        "\n",
        "# account for skew in the labeled data\n",
        "trainLabels = [int(p.split(os.path.sep)[-2]) for p in trainPaths]\n",
        "trainLabels = np_utils.to_categorical(trainLabels)\n",
        "classTotals = trainLabels.sum(axis=0)\n",
        "classWeight = classTotals.max() / classTotals\n",
        "\n",
        "# initialize the training training data augmentation object\n",
        "trainAug = ImageDataGenerator(\n",
        "\trescale=1 / 255.0,\n",
        "\trotation_range=20,\n",
        "\tzoom_range=0.05,\n",
        "\twidth_shift_range=0.1,\n",
        "\theight_shift_range=0.1,\n",
        "\tshear_range=0.05,\n",
        "\thorizontal_flip=True,\n",
        "\tvertical_flip=True,\n",
        "\tfill_mode=\"nearest\")\n",
        "\n",
        "# initialize the validation (and testing) data augmentation object\n",
        "valAug = ImageDataGenerator(rescale=1 / 255.0)\n",
        "\n",
        "# initialize the training generator\n",
        "trainGen = trainAug.flow_from_directory(\n",
        "\tconfigt.TRAIN_PATH,\n",
        "\tclass_mode=\"categorical\",\n",
        "\ttarget_size=(230, 230),\n",
        "\tcolor_mode=\"rgb\",\n",
        "\tshuffle=True,\n",
        "\tbatch_size=BS)\n",
        "\n",
        "# initialize the validation generator\n",
        "valGen = valAug.flow_from_directory(\n",
        "\tconfigt.VAL_PATH,\n",
        "\tclass_mode=\"categorical\",\n",
        "\ttarget_size=(230, 230),\n",
        "\tcolor_mode=\"rgb\",\n",
        "\tshuffle=False,\n",
        "\tbatch_size=BS )\n",
        "\n",
        "# initialize the testing generator\n",
        "testGen = valAug.flow_from_directory(\n",
        "\tconfigt.TEST_PATH,\n",
        "\tclass_mode=\"categorical\",\n",
        "\ttarget_size=(230, 230),\n",
        "\tcolor_mode=\"rgb\",\n",
        "\tshuffle=False,\n",
        "\tbatch_size=BS)\n",
        "\n",
        "# initialize our CancerNet model and compile it\n",
        "\n",
        "#opt = Adagrad(lr=INIT_LR, decay=INIT_LR / NUM_EPOCHS)\n",
        "#opt= Ranger(params=CancerNet.parameters(), lr=1e-3, alpha=0.5, k=6, N_sma_threshhold=5, betas=(.95,0.999), eps=1e-5, weight_decay=0)\n",
        "#opt = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "#opt = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=0.5, decay=INIT_LR / NUM_EPOCHS)\n",
        "#opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0)\n",
        "#opt = optimizers.Adadelta(lr=0.01, rho=0.95, epsilon=0.05, decay=1e-2)\n",
        "#opt = optimizers.Adamax(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "opt = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
        "ensemble_model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "\n",
        "# 绘制训练过程中的损失曲线和精度曲线\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "H = ensemble_model.fit_generator(\n",
        "\ttrainGen,\n",
        "\tsteps_per_epoch=totalTrain // BS,\n",
        "\tvalidation_data=valGen,\n",
        "\tvalidation_steps=totalVal // BS,\n",
        "\tclass_weight=classWeight,\n",
        "\tepochs=NUM_EPOCHS)\n",
        "\n",
        "# reset the testing generator and then use our trained model to\n",
        "# make predictions on the data\n",
        "print(\"[INFO] evaluating network...\")\n",
        "testGen.reset()\n",
        "predIdxs = ensemble_model.predict_generator(testGen,\n",
        "\tsteps=(totalTest // BS) + 1)\n",
        "\n",
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "# label with corresponding largest predicted probability\n",
        "ensemble_model.save(\"zuhe.h5\")\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "print(confusion_matrix(testGen.classes, predIdxs))\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(testGen.classes, predIdxs,\n",
        "\ttarget_names=testGen.class_indices.keys(), digits=4))\n",
        "\n",
        "# compute the confusion matrix and and use it to derive the raw\n",
        "# accuracy, sensitivity, and specificity\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = H.history['acc']\n",
        "val_acc = H.history['val_acc']\n",
        "loss = H.history['loss']\n",
        "val_loss = H.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'b:', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.plot(epochs, loss, 'b--', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r-.', label='Validation loss')\n",
        "plt.title('Training Loss and Accuracy on Dataset')\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# === 混淆矩阵：真实值与预测值的对比 ===\n",
        "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "con_mat = confusion_matrix(testGen.classes, predIdxs)\n",
        "\n",
        "con_mat_norm = con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis]     # 归一化\n",
        "\n",
        "con_mat_norm = np.around(con_mat_norm, decimals=4)\n",
        "\n",
        "# === plot ===\n",
        "plt.figure(figsize=(8, 8))\n",
        "sns.heatmap(con_mat_norm, annot=True, cmap='Blues')\n",
        "\n",
        "plt.ylim(0, 8)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qTVPf1xRDq7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.1 pytorch\n",
        "[paper new final](https://www.techscience.com/ueditor/files/cmes/TSP_CMES-130-2/TSP_CMES_17030/TSP_CMES_17030.pdf)"
      ],
      "metadata": {
        "id": "aI8hYDpInBnt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWrBdwv8CNtj",
        "outputId": "974b7165-ba0b-4b1e-9f1c-f45027a92403"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfaEReq0RN1Z",
        "outputId": "bc58d55f-46f6-497d-b2a4-7d257410ccc9"
      },
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import Subset, DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.transforms import Compose, ToTensor, Resize\n",
        "from torch.optim.lr_scheduler import ExponentialLR, StepLR, ReduceLROnPlateau\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\n",
        "\n",
        "\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "plt.ion()   # interactive mode"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version:  1.10.0+cu111\n",
            "Torchvision Version:  0.11.1+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgvdLq1MzFau"
      },
      "source": [
        "TRAIN_DIR =  '/content/drive/My Drive/BreaKHis2_8_trial/BreaKHis8_train/' #where train total : 1600 (400 per folder)\n",
        "VAL_DIR =  '/content/drive/My Drive/BreaKHis2_8_trial/BreaKHis8_val/' #where val total : 80 (20 per folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8y1-Cf_LQi5J"
      },
      "source": [
        "data_transform1 = transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor()])\n",
        "        #transforms.Normalize(mean=[0.5, 0.5, 0.5],     std=[0.5, 0.5, 0.5]) # transforms.Normalize(mean=[0.8513, 0.6445, 0.8938],std=[0.1047, 0.1634, 0.0739]) #\n",
        "\n",
        "\n",
        "data_transform2 = transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        #transforms.RandomRotation(90),\n",
        "        #transforms.RandomHorizontalFlip(0.8),\n",
        "        transforms.ToTensor()])\n",
        "        #transforms.Normalize(mean=[0.5, 0.5, 0.5],     std=[0.5, 0.5, 0.5]) #transforms.Normalize(mean=[0.8513, 0.6445, 0.8938],  std=[0.1047, 0.1634, 0.0739])\n",
        "\n",
        "\n",
        "train_orig = torchvision.datasets.ImageFolder(TRAIN_DIR,\n",
        "                                           transform=data_transform1)\n",
        "\n",
        "#train_aug = torchvision.datasets.ImageFolder(TRAIN_DIR,\n",
        " #                                          transform=data_transform2)\n",
        "\n",
        "\n",
        "#increased_train = torch.utils.data.ConcatDataset([train_aug,train_orig])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU-bs2tGhBc4",
        "outputId": "4d4e55f7-5d03-4812-d230-7cd54b258b8a"
      },
      "source": [
        "len(train_orig)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "480"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLE68feOsSdV"
      },
      "source": [
        "val_orig = torchvision.datasets.ImageFolder(VAL_DIR,\n",
        "                                           transform=data_transform1)\n",
        "\n",
        "#val_aug = torchvision.datasets.ImageFolder(VAL_DIR,\n",
        " #                                          transform=data_transform2)\n",
        "\n",
        "\n",
        "#increased_val = torch.utils.data.ConcatDataset([val_aug,val_orig])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5eQFYwEsSmc",
        "outputId": "c12aafb1-a295-4d26-8c24-f2d910a63369"
      },
      "source": [
        "len(val_orig)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih7Xg_H0y3Sz"
      },
      "source": [
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
        "model_name = \"resnet\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 8\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 16\n",
        "\n",
        "# Number of epochs to train\n",
        "num_epochs = 80\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "#   when True we only update the reshaped layer params\n",
        "feature_extract = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTDkd0Z-GBAI"
      },
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output. In train\n",
        "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                    #   but in testing we only consider the final output.\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            #scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIXhYYFaELM4"
      },
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b4250d5b81834ad083b0cd7c2a904a8c",
            "32d0231b4c9d4c81ae9995ac2b272f1f",
            "2399a30170d74966b01c3353a2f290df",
            "e29ba468730f4497bb5e2f1ca9b60091",
            "be1194b6390c4c87a653cedd6a7ad559",
            "1bd85d17e3be46cba3db6e5553e13c82",
            "f2571936709442669929a7a57a9f67ce",
            "ce6ef29f29be454291f7812e306eb468",
            "173eebea58c44849b26f357e65a25daa",
            "781bd2455e4248b4b5bc80ac5bd53e03",
            "c5ce94dc489b4ac6849c71249a1c249d"
          ]
        },
        "id": "f3pdLBAiHnJZ",
        "outputId": "e1ae5584-7796-47bf-a1c2-9c8df4ca91f4"
      },
      "source": [
        "\n",
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet50(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Sequential(\n",
        "                        nn.Linear(num_ftrs, 256),\n",
        "                        nn.ReLU(),\n",
        "                        #nn.Dropout(0.4),                             #dropout here\n",
        "                        nn.Linear(256, num_classes),\n",
        "                        nn.Softmax(dim=1))\n",
        "        #model_ft.fc = nn.Dropout(0.4)\n",
        "        #model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "        #model_ft.classifier = nn.Sequential(\n",
        "         #               nn.Linear(num_ftrs, 256),\n",
        "          #              nn.ReLU(),\n",
        "           #             nn.Dropout(0.4),                          #dropout here\n",
        "            #            nn.Linear(256, num_classes))\n",
        "        #model_ft.classifier = nn.Dropout(0.3)\n",
        "        #model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"inception\":\n",
        "        \"\"\" Inception v3\n",
        "        Be careful, expects (299,299) sized images and has auxiliary output\n",
        "        \"\"\"\n",
        "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        # Handle the auxilary net\n",
        "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        # Handle the primary net\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 299\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "\n",
        "    return model_ft, input_size\n",
        "\n",
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "# Print the model we just instantiated\n",
        "print(model_ft)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4250d5b81834ad083b0cd7c2a904a8c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=2048, out_features=256, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=256, out_features=8, bias=True)\n",
            "    (3): Softmax(dim=1)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpjWsAxBGBES"
      },
      "source": [
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "train_loader = data.DataLoader(train_orig, batch_size=batch_size, shuffle=True,  num_workers=4)\n",
        "val_loader  = data.DataLoader(val_orig, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "\n",
        "dataloaders_dict = {'train':train_loader, 'val':val_loader}\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9pXvdOgS51-",
        "outputId": "b07b3217-b1b7-4de9-8b93-c3aff4f15734"
      },
      "source": [
        "# Send the model to GPU\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Gather the parameters to be optimized/updated in this run. If we are\n",
        "#  finetuning we will be updating all parameters. However, if we are\n",
        "#  doing feature extract method, we will only update the parameters\n",
        "#  that we have just initialized, i.e. the parameters with requires_grad\n",
        "#  is True.\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params to learn:\n",
            "\t fc.0.weight\n",
            "\t fc.0.bias\n",
            "\t fc.2.weight\n",
            "\t fc.2.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3zchgG6NXqn"
      },
      "source": [
        "#scheduler2 = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
        "\n",
        "optimizer_ft = optim.Adam(params_to_update, lr=0.00001, betas=(0.9, 0.999), eps=1e-08, amsgrad=False)#optim.SGD(params_to_update, lr=0.001, momentum=0.9, weight_decay=1e-2) #\n",
        "#schedulr = ExponentialLR(optimizer_ft, gamma=1)#StepLR(optimizer_ft, step_size=20, gamma=1) #ReduceLROnPlateau(optimizer_ft, mode='min', factor=0.1, patience=2, threshold=0.0001)#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DFrEyoxTAm3",
        "outputId": "db786c19-0043-4db0-b6fe-771da56ea76b"
      },
      "source": [
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate\n",
        "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/79\n",
            "----------\n",
            "train Loss: 2.0788 Acc: 0.1167\n",
            "val Loss: 2.0797 Acc: 0.1250\n",
            "\n",
            "Epoch 1/79\n",
            "----------\n",
            "train Loss: 2.0760 Acc: 0.1750\n",
            "val Loss: 2.0799 Acc: 0.1250\n",
            "\n",
            "Epoch 2/79\n",
            "----------\n",
            "train Loss: 2.0738 Acc: 0.2375\n",
            "val Loss: 2.0789 Acc: 0.0750\n",
            "\n",
            "Epoch 3/79\n",
            "----------\n",
            "train Loss: 2.0717 Acc: 0.2729\n",
            "val Loss: 2.0777 Acc: 0.1250\n",
            "\n",
            "Epoch 4/79\n",
            "----------\n",
            "train Loss: 2.0695 Acc: 0.2542\n",
            "val Loss: 2.0768 Acc: 0.1500\n",
            "\n",
            "Epoch 5/79\n",
            "----------\n",
            "train Loss: 2.0668 Acc: 0.2583\n",
            "val Loss: 2.0760 Acc: 0.1250\n",
            "\n",
            "Epoch 6/79\n",
            "----------\n",
            "train Loss: 2.0646 Acc: 0.2313\n",
            "val Loss: 2.0755 Acc: 0.1000\n",
            "\n",
            "Epoch 7/79\n",
            "----------\n",
            "train Loss: 2.0617 Acc: 0.2313\n",
            "val Loss: 2.0743 Acc: 0.1500\n",
            "\n",
            "Epoch 8/79\n",
            "----------\n",
            "train Loss: 2.0592 Acc: 0.2542\n",
            "val Loss: 2.0736 Acc: 0.1250\n",
            "\n",
            "Epoch 9/79\n",
            "----------\n",
            "train Loss: 2.0572 Acc: 0.2271\n",
            "val Loss: 2.0723 Acc: 0.1250\n",
            "\n",
            "Epoch 10/79\n",
            "----------\n",
            "train Loss: 2.0542 Acc: 0.2521\n",
            "val Loss: 2.0713 Acc: 0.1250\n",
            "\n",
            "Epoch 11/79\n",
            "----------\n",
            "train Loss: 2.0501 Acc: 0.2646\n",
            "val Loss: 2.0698 Acc: 0.1250\n",
            "\n",
            "Epoch 12/79\n",
            "----------\n",
            "train Loss: 2.0472 Acc: 0.2646\n",
            "val Loss: 2.0693 Acc: 0.1750\n",
            "\n",
            "Epoch 13/79\n",
            "----------\n",
            "train Loss: 2.0431 Acc: 0.2687\n",
            "val Loss: 2.0680 Acc: 0.1250\n",
            "\n",
            "Epoch 14/79\n",
            "----------\n",
            "train Loss: 2.0410 Acc: 0.2896\n",
            "val Loss: 2.0655 Acc: 0.2000\n",
            "\n",
            "Epoch 15/79\n",
            "----------\n",
            "train Loss: 2.0383 Acc: 0.2646\n",
            "val Loss: 2.0654 Acc: 0.1500\n",
            "\n",
            "Epoch 16/79\n",
            "----------\n",
            "train Loss: 2.0333 Acc: 0.2667\n",
            "val Loss: 2.0646 Acc: 0.1750\n",
            "\n",
            "Epoch 17/79\n",
            "----------\n",
            "train Loss: 2.0265 Acc: 0.2958\n",
            "val Loss: 2.0630 Acc: 0.2250\n",
            "\n",
            "Epoch 18/79\n",
            "----------\n",
            "train Loss: 2.0236 Acc: 0.2875\n",
            "val Loss: 2.0624 Acc: 0.2000\n",
            "\n",
            "Epoch 19/79\n",
            "----------\n",
            "train Loss: 2.0237 Acc: 0.2875\n",
            "val Loss: 2.0610 Acc: 0.2250\n",
            "\n",
            "Epoch 20/79\n",
            "----------\n",
            "train Loss: 2.0163 Acc: 0.3167\n",
            "val Loss: 2.0593 Acc: 0.2250\n",
            "\n",
            "Epoch 21/79\n",
            "----------\n",
            "train Loss: 2.0143 Acc: 0.2979\n",
            "val Loss: 2.0571 Acc: 0.2250\n",
            "\n",
            "Epoch 22/79\n",
            "----------\n",
            "train Loss: 2.0084 Acc: 0.3229\n",
            "val Loss: 2.0566 Acc: 0.2250\n",
            "\n",
            "Epoch 23/79\n",
            "----------\n",
            "train Loss: 2.0044 Acc: 0.3271\n",
            "val Loss: 2.0552 Acc: 0.2250\n",
            "\n",
            "Epoch 24/79\n",
            "----------\n",
            "train Loss: 2.0001 Acc: 0.3438\n",
            "val Loss: 2.0547 Acc: 0.2250\n",
            "\n",
            "Epoch 25/79\n",
            "----------\n",
            "train Loss: 1.9969 Acc: 0.3708\n",
            "val Loss: 2.0520 Acc: 0.2000\n",
            "\n",
            "Epoch 26/79\n",
            "----------\n",
            "train Loss: 1.9908 Acc: 0.3521\n",
            "val Loss: 2.0510 Acc: 0.2000\n",
            "\n",
            "Epoch 27/79\n",
            "----------\n",
            "train Loss: 1.9888 Acc: 0.3583\n",
            "val Loss: 2.0511 Acc: 0.2250\n",
            "\n",
            "Epoch 28/79\n",
            "----------\n",
            "train Loss: 1.9806 Acc: 0.3625\n",
            "val Loss: 2.0501 Acc: 0.2250\n",
            "\n",
            "Epoch 29/79\n",
            "----------\n",
            "train Loss: 1.9807 Acc: 0.3979\n",
            "val Loss: 2.0451 Acc: 0.2500\n",
            "\n",
            "Epoch 30/79\n",
            "----------\n",
            "train Loss: 1.9756 Acc: 0.3979\n",
            "val Loss: 2.0448 Acc: 0.2500\n",
            "\n",
            "Epoch 31/79\n",
            "----------\n",
            "train Loss: 1.9729 Acc: 0.3896\n",
            "val Loss: 2.0426 Acc: 0.2250\n",
            "\n",
            "Epoch 32/79\n",
            "----------\n",
            "train Loss: 1.9704 Acc: 0.3833\n",
            "val Loss: 2.0425 Acc: 0.2500\n",
            "\n",
            "Epoch 33/79\n",
            "----------\n",
            "train Loss: 1.9606 Acc: 0.4146\n",
            "val Loss: 2.0414 Acc: 0.2500\n",
            "\n",
            "Epoch 34/79\n",
            "----------\n",
            "train Loss: 1.9562 Acc: 0.4042\n",
            "val Loss: 2.0386 Acc: 0.2250\n",
            "\n",
            "Epoch 35/79\n",
            "----------\n",
            "train Loss: 1.9580 Acc: 0.4188\n",
            "val Loss: 2.0350 Acc: 0.2750\n",
            "\n",
            "Epoch 36/79\n",
            "----------\n",
            "train Loss: 1.9531 Acc: 0.4083\n",
            "val Loss: 2.0378 Acc: 0.2500\n",
            "\n",
            "Epoch 37/79\n",
            "----------\n",
            "train Loss: 1.9498 Acc: 0.4083\n",
            "val Loss: 2.0339 Acc: 0.2750\n",
            "\n",
            "Epoch 38/79\n",
            "----------\n",
            "train Loss: 1.9485 Acc: 0.4375\n",
            "val Loss: 2.0313 Acc: 0.3000\n",
            "\n",
            "Epoch 39/79\n",
            "----------\n",
            "train Loss: 1.9444 Acc: 0.4333\n",
            "val Loss: 2.0288 Acc: 0.3000\n",
            "\n",
            "Epoch 40/79\n",
            "----------\n",
            "train Loss: 1.9352 Acc: 0.4500\n",
            "val Loss: 2.0304 Acc: 0.3000\n",
            "\n",
            "Epoch 41/79\n",
            "----------\n",
            "train Loss: 1.9325 Acc: 0.4417\n",
            "val Loss: 2.0286 Acc: 0.3000\n",
            "\n",
            "Epoch 42/79\n",
            "----------\n",
            "train Loss: 1.9292 Acc: 0.4333\n",
            "val Loss: 2.0259 Acc: 0.3000\n",
            "\n",
            "Epoch 43/79\n",
            "----------\n",
            "train Loss: 1.9284 Acc: 0.4437\n",
            "val Loss: 2.0237 Acc: 0.3250\n",
            "\n",
            "Epoch 44/79\n",
            "----------\n",
            "train Loss: 1.9294 Acc: 0.4250\n",
            "val Loss: 2.0231 Acc: 0.3000\n",
            "\n",
            "Epoch 45/79\n",
            "----------\n",
            "train Loss: 1.9226 Acc: 0.4437\n",
            "val Loss: 2.0224 Acc: 0.3250\n",
            "\n",
            "Epoch 46/79\n",
            "----------\n",
            "train Loss: 1.9162 Acc: 0.4667\n",
            "val Loss: 2.0194 Acc: 0.3500\n",
            "\n",
            "Epoch 47/79\n",
            "----------\n",
            "train Loss: 1.9224 Acc: 0.4458\n",
            "val Loss: 2.0197 Acc: 0.3000\n",
            "\n",
            "Epoch 48/79\n",
            "----------\n",
            "train Loss: 1.9153 Acc: 0.4417\n",
            "val Loss: 2.0197 Acc: 0.3000\n",
            "\n",
            "Epoch 49/79\n",
            "----------\n",
            "train Loss: 1.9131 Acc: 0.4583\n",
            "val Loss: 2.0161 Acc: 0.3250\n",
            "\n",
            "Epoch 50/79\n",
            "----------\n",
            "train Loss: 1.9024 Acc: 0.4750\n",
            "val Loss: 2.0143 Acc: 0.3500\n",
            "\n",
            "Epoch 51/79\n",
            "----------\n",
            "train Loss: 1.8987 Acc: 0.4667\n",
            "val Loss: 2.0132 Acc: 0.3250\n",
            "\n",
            "Epoch 52/79\n",
            "----------\n",
            "train Loss: 1.8996 Acc: 0.4479\n",
            "val Loss: 2.0138 Acc: 0.2750\n",
            "\n",
            "Epoch 53/79\n",
            "----------\n",
            "train Loss: 1.8970 Acc: 0.4750\n",
            "val Loss: 2.0132 Acc: 0.3250\n",
            "\n",
            "Epoch 54/79\n",
            "----------\n",
            "train Loss: 1.8863 Acc: 0.4688\n",
            "val Loss: 2.0091 Acc: 0.3500\n",
            "\n",
            "Epoch 55/79\n",
            "----------\n",
            "train Loss: 1.8984 Acc: 0.4583\n",
            "val Loss: 2.0093 Acc: 0.3000\n",
            "\n",
            "Epoch 56/79\n",
            "----------\n",
            "train Loss: 1.8951 Acc: 0.4542\n",
            "val Loss: 2.0103 Acc: 0.3500\n",
            "\n",
            "Epoch 57/79\n",
            "----------\n",
            "train Loss: 1.8954 Acc: 0.4729\n",
            "val Loss: 2.0048 Acc: 0.3250\n",
            "\n",
            "Epoch 58/79\n",
            "----------\n",
            "train Loss: 1.8819 Acc: 0.4875\n",
            "val Loss: 2.0098 Acc: 0.3000\n",
            "\n",
            "Epoch 59/79\n",
            "----------\n",
            "train Loss: 1.8866 Acc: 0.4708\n",
            "val Loss: 2.0049 Acc: 0.3500\n",
            "\n",
            "Epoch 60/79\n",
            "----------\n",
            "train Loss: 1.8825 Acc: 0.4813\n",
            "val Loss: 2.0037 Acc: 0.3500\n",
            "\n",
            "Epoch 61/79\n",
            "----------\n",
            "train Loss: 1.8765 Acc: 0.4896\n",
            "val Loss: 2.0004 Acc: 0.3250\n",
            "\n",
            "Epoch 62/79\n",
            "----------\n",
            "train Loss: 1.8808 Acc: 0.4646\n",
            "val Loss: 2.0022 Acc: 0.3500\n",
            "\n",
            "Epoch 63/79\n",
            "----------\n",
            "train Loss: 1.8796 Acc: 0.4542\n",
            "val Loss: 2.0010 Acc: 0.3500\n",
            "\n",
            "Epoch 64/79\n",
            "----------\n",
            "train Loss: 1.8718 Acc: 0.4854\n",
            "val Loss: 1.9972 Acc: 0.3500\n",
            "\n",
            "Epoch 65/79\n",
            "----------\n",
            "train Loss: 1.8725 Acc: 0.4708\n",
            "val Loss: 2.0008 Acc: 0.3500\n",
            "\n",
            "Epoch 66/79\n",
            "----------\n",
            "train Loss: 1.8747 Acc: 0.4708\n",
            "val Loss: 1.9951 Acc: 0.3250\n",
            "\n",
            "Epoch 67/79\n",
            "----------\n",
            "train Loss: 1.8681 Acc: 0.4688\n",
            "val Loss: 1.9978 Acc: 0.3500\n",
            "\n",
            "Epoch 68/79\n",
            "----------\n",
            "train Loss: 1.8599 Acc: 0.4958\n",
            "val Loss: 2.0007 Acc: 0.3500\n",
            "\n",
            "Epoch 69/79\n",
            "----------\n",
            "train Loss: 1.8542 Acc: 0.4854\n",
            "val Loss: 1.9930 Acc: 0.3750\n",
            "\n",
            "Epoch 70/79\n",
            "----------\n",
            "train Loss: 1.8625 Acc: 0.4813\n",
            "val Loss: 1.9930 Acc: 0.3500\n",
            "\n",
            "Epoch 71/79\n",
            "----------\n",
            "train Loss: 1.8583 Acc: 0.4917\n",
            "val Loss: 1.9917 Acc: 0.3750\n",
            "\n",
            "Epoch 72/79\n",
            "----------\n",
            "train Loss: 1.8644 Acc: 0.4896\n",
            "val Loss: 1.9906 Acc: 0.3250\n",
            "\n",
            "Epoch 73/79\n",
            "----------\n",
            "train Loss: 1.8628 Acc: 0.4875\n",
            "val Loss: 1.9916 Acc: 0.3500\n",
            "\n",
            "Epoch 74/79\n",
            "----------\n",
            "train Loss: 1.8459 Acc: 0.5083\n",
            "val Loss: 1.9918 Acc: 0.3500\n",
            "\n",
            "Epoch 75/79\n",
            "----------\n",
            "train Loss: 1.8504 Acc: 0.4917\n",
            "val Loss: 1.9899 Acc: 0.3500\n",
            "\n",
            "Epoch 76/79\n",
            "----------\n",
            "train Loss: 1.8465 Acc: 0.5125\n",
            "val Loss: 1.9883 Acc: 0.3500\n",
            "\n",
            "Epoch 77/79\n",
            "----------\n",
            "train Loss: 1.8483 Acc: 0.4979\n",
            "val Loss: 1.9856 Acc: 0.3500\n",
            "\n",
            "Epoch 78/79\n",
            "----------\n",
            "train Loss: 1.8522 Acc: 0.5062\n",
            "val Loss: 1.9875 Acc: 0.3500\n",
            "\n",
            "Epoch 79/79\n",
            "----------\n",
            "train Loss: 1.8468 Acc: 0.4896\n",
            "val Loss: 1.9855 Acc: 0.3500\n",
            "\n",
            "Training complete in 265m 32s\n",
            "Best val Acc: 0.375000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ5mmdN2Kv8m"
      },
      "source": [
        "MODEL_DIR= '/content/drive/My Drive/FEW_SHOT_MULTI_TASK/Comparison/'\n",
        "torch.save(model_ft, MODEL_DIR + 'Paper2_new_final_resnet50.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyFd-KEALFOp",
        "outputId": "fb92faab-ba71-4abc-a66a-a9e924527754"
      },
      "source": [
        "MODEL_DIR= '/content/drive/My Drive/FEW_SHOT_MULTI_TASK/Comparison/'\n",
        "\n",
        "#model = models.resnet18(pretrained=True)\n",
        "#num_ftrs = model.fc.in_features\n",
        "#model.fc = nn.Linear(num_ftrs, 4)  # make the change\n",
        "\n",
        "#model.load_state_dict(torch.load(MODEL_DIR + 'resnet_model.pt' ))\n",
        "model = torch.load(MODEL_DIR + 'Paper2_new_final_resnet50.pt')\n",
        "model.eval()\n",
        "#device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
        "\n",
        "#model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=2048, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=256, out_features=8, bias=True)\n",
              "    (3): Softmax(dim=1)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2RX9yRPSYhw"
      },
      "source": [
        "\n",
        "TEST_DIR = '/content/drive/My Drive/BreaKHis2_8_trial/BreaKHis8_test/'\n",
        "test_transform = transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor()\n",
        "        #transforms.Normalize(mean=[0.5, 0.5, 0.5],     std=[0.5, 0.5, 0.5])#         transforms.Normalize(mean=[0.8513, 0.6445, 0.8938],std=[0.1047, 0.1634, 0.0739])         #\n",
        "    ])\n",
        "test_ds = torchvision.datasets.ImageFolder(TEST_DIR,\n",
        "                                           transform=test_transform)\n",
        "test_loader = data.DataLoader(test_ds, batch_size=batch_size, shuffle=True, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9NakwzphdIS"
      },
      "source": [
        "def check_accuracy(loader, model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
        "\n",
        "    model.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiubW584hd8E",
        "outputId": "a9963118-b85c-47c9-bc53-063cc186c21f"
      },
      "source": [
        "check_accuracy(test_loader, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 310 / 1876 with accuracy 16.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_accuracy(loader, model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)\n",
        "            #num_correct += (predictions == y).sum()\n",
        "            #num_samples += predictions.size(0)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "gMcdH6uQnD8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prd = check_accuracy(test_loader, model)"
      ],
      "metadata": {
        "id": "1YdN2bFeuDvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prd.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kx41QWHQnEAY",
        "outputId": "cf1f058b-c112-49d3-b806-eb8279ad7d1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_features, test_labels = next(iter(test_loader))"
      ],
      "metadata": {
        "id": "N_MCvz_PnEB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YUQZVt4nEHQ",
        "outputId": "ea5c49aa-0483-44af-f98b-513624c833f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4, 3, 1, 5, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 2, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQsYpFyynEJE",
        "outputId": "24c871e0-f2c8-4c88-a250-ef4db87dc54a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "classification report"
      ],
      "metadata": {
        "id": "DIoTlARhxuQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "\n",
        "#importing accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(test_labels, prd)))\n",
        "\n",
        "print('Micro Precision: {:.2f}'.format(precision_score(test_labels, prd, average='micro')))\n",
        "print('Micro Recall: {:.2f}'.format(recall_score(test_labels, prd, average='micro')))\n",
        "print('Micro F1-score: {:.2f}\\n'.format(f1_score(test_labels, prd, average='micro')))\n",
        "\n",
        "print('Macro Precision: {:.2f}'.format(precision_score(test_labels, prd, average='macro')))\n",
        "print('Macro Recall: {:.2f}'.format(recall_score(test_labels, prd, average='macro')))\n",
        "print('Macro F1-score: {:.2f}\\n'.format(f1_score(test_labels, prd, average='macro')))\n",
        "\n",
        "print('Weighted Precision: {:.2f}'.format(precision_score(test_labels, prd, average='weighted')))\n",
        "print('Weighted Recall: {:.2f}'.format(recall_score(test_labels, prd, average='weighted')))\n",
        "print('Weighted F1-score: {:.2f}'.format(f1_score(test_labels, prd, average='weighted')))\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print('\\nClassification Report\\n')\n",
        "print(classification_report(test_labels, prd, target_names=['Class 0- A:', 'Class 1- F:', 'Class 2- PT:', 'Class 3- TA:','Class 4- DC:','Class 5- LC:','Class 6- MC:', 'Class 7- PC:'], digits = 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zh6AOqi1nEMm",
        "outputId": "9dfb67ac-6fa5-4803-b216-9c0f8e65a00b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-005498094edd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#importing accuracy_score, precision_score, recall_score, f1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nAccuracy: {:.2f}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Micro Precision: {:.2f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    332\u001b[0m         raise ValueError(\n\u001b[1;32m    333\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         )\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [16, 4]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUC scores"
      ],
      "metadata": {
        "id": "AlaKRik8xvDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "#roc auc score\n",
        "scr1 = roc_auc_score(test_labels, pred, multi_class='ovo', average='macro')\n",
        "scr3 = roc_auc_score(test_labels, pred, multi_class='ovr', average='macro')\n",
        "scr2 = roc_auc_score(test_labels, pred, multi_class='ovo', average='weighted')\n",
        "scr4 = roc_auc_score(test_labels, pred, multi_class='ovr', average='weighted')\n",
        "\n",
        "print(\"macro - ovo ROC AUC Score: \", scr1)\n",
        "print(\"\")\n",
        "\n",
        "print(\"weighted - ovo ROC AUC Score: \", scr2)\n",
        "print(\"\")\n",
        "\n",
        "print(\"macro - ovr ROC AUC Score: \", scr3)\n",
        "print(\"\")\n",
        "\n",
        "print(\"weighted - ovr ROC AUC Score: \", scr4)\n",
        "print(\"\")"
      ],
      "metadata": {
        "id": "M7ufpoNDxtdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "clss wise ROC curves and AUC scores"
      ],
      "metadata": {
        "id": "0wZ_D5kpx1ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# roc curve for classes\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "thresh ={}\n",
        "\n",
        "target_names = ['A', 'F', 'PT', 'TA', 'DC','LC', 'MC', 'PC']\n",
        "n_class = 8\n",
        "\n",
        "for i in range(len(target_names)):\n",
        "    fpr[i], tpr[i], thresh[i] = roc_curve(test_labels, prd[:], pos_label=i)\n",
        "    auroc = round(metrics.auc(fpr[i], tpr[i]),2)\n",
        "    print('class',i,'-', target_names[i],' :','--AUC--->',auroc)\n",
        "\n",
        "# plotting\n",
        "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0-A: vs Rest')\n",
        "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1- F: vs Rest')\n",
        "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2- PT: vs Rest')\n",
        "plt.plot(fpr[3], tpr[3], linestyle='--',color='yellow', label='Class 3- TA: vs Rest')\n",
        "plt.plot(fpr[4], tpr[4], linestyle='-',color='orange', label='Class 4-DC vs Rest')\n",
        "plt.plot(fpr[5], tpr[5], linestyle='-',color='green', label='Class 5-LC: vs Rest')\n",
        "plt.plot(fpr[6], tpr[6], linestyle='-',color='blue', label='Class 6-MC: vs Rest')\n",
        "plt.plot(fpr[7], tpr[7], linestyle='-',color='yellow', label='Class 7- LC: vs Rest')\n",
        "\n",
        "\n",
        "plt.title('Multiclass ROC-AUC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive rate')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('Multilass ROC',dpi=300, bbox_inches='tight',   pad_inches = 0);\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pTTy8n0Sxtey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# final 2.1 keras\n",
        "[paper new final](https://www.techscience.com/ueditor/files/cmes/TSP_CMES-130-2/TSP_CMES_17030/TSP_CMES_17030.pdf)"
      ],
      "metadata": {
        "id": "x1hFXYTjgo3C"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4182b293-c713-45d0-f9d9-9a5261685e59",
        "id": "ffoCx3gggqGg"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import argparse\n",
        "#from myconfig import configt\n",
        "from keras import optimizers\n",
        "from keras.utils import np_utils\n",
        "#config = tf.ConfigProto()\n",
        "#config.gpu_options.allow_growth = True\n",
        "#keras.backend.tensorflow_backend.set_session(tf.Session(config=config))\n",
        "\n",
        "#conv_base = ResNet50(weights='imagenet', include_top=False, input_shape=(230, 230, 3))\n",
        "#from keras.applications.xception import Xception\n",
        "#conv_base = Xception(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
        "\n",
        "\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import numpy as np\n",
        "from imutils import paths\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "from keras.layers import Average\n",
        "from keras.layers import merge\n",
        "\n",
        "from keras import applications\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "from keras.models import Model, Input\n",
        "\n",
        "import argparse\n",
        "\n",
        "from keras import optimizers\n",
        "import seaborn as sns\n",
        "#from keras.engine.topology import Input\n"
      ],
      "metadata": {
        "id": "F0jue8uDg29s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import math\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from keras import layers\n",
        "from tensorflow.keras.applications import ResNet50,MobileNet, DenseNet201, InceptionV3, NASNetLarge, InceptionResNetV2, NASNetMobile\n",
        "from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
        "import scipy\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "import gc\n",
        "from functools import partial\n",
        "from sklearn import metrics\n",
        "from collections import Counter\n",
        "import json\n",
        "import itertools\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "SqXsSOdVg2-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the matplotlib backend so figures can be saved in the background\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "# import the necessary packages\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from sklearn.metrics import classification_report\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse"
      ],
      "metadata": {
        "id": "eBUwgB6Kg3C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale = 1./255)\n",
        "       # rotation_range=40,\n",
        "       # height_shift_range=0.2,\n",
        "       # shear_range=0.2,\n",
        "       # zoom_range=0.2,\n",
        "       # horizontal_flip=True,\n",
        "       # fill_mode='nearest')\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "2lh0pvwUg3Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainGen = train_datagen.flow_from_directory(\n",
        "    directory=\"/content/drive/My Drive/BreaKHis2_8_trial/BreaKHis8_train/\",\n",
        "    target_size=(224, 224),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=4,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False,\n",
        "    seed=42\n",
        ")\n",
        "valGen = valid_datagen.flow_from_directory(\n",
        "    directory=\"/content/drive/My Drive/BreaKHis2_8_trial/BreaKHis8_val/\",\n",
        "    target_size=(224, 224),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=4,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False,\n",
        "    seed=42\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiZZBwExg3Ik",
        "outputId": "c553235c-02bd-4f53-8549-af6f53a9ff44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 480 images belonging to 8 classes.\n",
            "Found 40 images belonging to 8 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"label map for train: \",trainGen.class_indices)\n",
        "print(\"\")\n",
        "print(\"----\")\n",
        "print(\"\")\n",
        "print(\"label map for val data: \",valGen.class_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRNrNhXmhJ1S",
        "outputId": "27b93d14-6387-4966-888e-1fb5b49de3fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label map for train:  {'benign_adenosis': 0, 'benign_fibroadenoma': 1, 'benign_phyllodestumor': 2, 'benign_tubularadenoma': 3, 'malignant_ductalcarcinoma': 4, 'malignant_lobularcarcinoma': 5, 'malignant_mucinouscarcinoma': 6, 'malignant_papillarycarcinoma': 7}\n",
            "\n",
            "----\n",
            "\n",
            "label map for val data:  {'benign_adenosis': 0, 'benign_fibroadenoma': 1, 'benign_phyllodestumor': 2, 'benign_tubularadenoma': 3, 'malignant_ductalcarcinoma': 4, 'malignant_lobularcarcinoma': 5, 'malignant_mucinouscarcinoma': 6, 'malignant_papillarycarcinoma': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train classes: \",trainGen.classes)\n",
        "print(\"\")\n",
        "print(\"Valid classes: \",valGen.classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jEJp0QFhJ2D",
        "outputId": "09937c32-80dd-46b1-975c-9afb8351b8ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train classes:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3\n",
            " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
            " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6\n",
            " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
            " 6 6 6 6 6 6 6 6 6 6 6 6 6 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
            " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n",
            "\n",
            "Valid classes:  [0 0 0 0 0 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 4 4 4 4 4 5 5 5 5 5 6 6 6 6 6 7 7\n",
            " 7 7 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "djXJ9DHChQWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testGen = test_datagen.flow_from_directory(\n",
        "    directory=\"/content/drive/My Drive/BreaKHis2_8_trial/BreaKHis8_test/\",\n",
        "    target_size=(224, 224),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=1,\n",
        "    class_mode=\"categorical\",  #None\n",
        "    shuffle=False,\n",
        "    #seed=42\n",
        ")\n",
        "nBatches = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uead8M6phRRQ",
        "outputId": "3d6436b9-7ffb-4380-d3d2-109a262b0f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1876 images belonging to 8 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[INFO] preparing model...\")\n",
        "baseModel = ResNet50(weights=\"imagenet\", include_top=False,\n",
        "\tinput_tensor=Input(shape=(224, 224, 3)))\n",
        "baseModel.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WW91W9zhdkv",
        "outputId": "3e3be5cb-07c7-4f56-ba92-9aad1cd6aac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] preparing model...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n",
            "94781440/94765736 [==============================] - 1s 0us/step\n",
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv1_pad (ZeroPadding2D)      (None, 230, 230, 3)  0           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv1_conv (Conv2D)            (None, 112, 112, 64  9472        ['conv1_pad[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv1_bn (BatchNormalization)  (None, 112, 112, 64  256         ['conv1_conv[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv1_relu (Activation)        (None, 112, 112, 64  0           ['conv1_bn[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " pool1_pad (ZeroPadding2D)      (None, 114, 114, 64  0           ['conv1_relu[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " pool1_pool (MaxPooling2D)      (None, 56, 56, 64)   0           ['pool1_pad[0][0]']              \n",
            "                                                                                                  \n",
            " conv2_block1_1_conv (Conv2D)   (None, 56, 56, 64)   4160        ['pool1_pool[0][0]']             \n",
            "                                                                                                  \n",
            " conv2_block1_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block1_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_0_conv (Conv2D)   (None, 56, 56, 256)  16640       ['pool1_pool[0][0]']             \n",
            "                                                                                                  \n",
            " conv2_block1_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block1_0_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
            "                                                                  'conv2_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block1_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block2_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block2_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block2_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block2_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_out[0][0]',       \n",
            "                                                                  'conv2_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block2_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block3_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block3_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block3_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block3_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_add (Add)         (None, 56, 56, 256)  0           ['conv2_block2_out[0][0]',       \n",
            "                                                                  'conv2_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block3_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_1_conv (Conv2D)   (None, 28, 28, 128)  32896       ['conv2_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block1_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_0_conv (Conv2D)   (None, 28, 28, 512)  131584      ['conv2_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block1_0_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
            "                                                                  'conv3_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block1_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block2_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block2_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block2_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block2_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_out[0][0]',       \n",
            "                                                                  'conv3_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block2_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block3_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block3_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block3_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block3_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_add (Add)         (None, 28, 28, 512)  0           ['conv3_block2_out[0][0]',       \n",
            "                                                                  'conv3_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block3_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block4_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block4_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block4_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block4_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_add (Add)         (None, 28, 28, 512)  0           ['conv3_block3_out[0][0]',       \n",
            "                                                                  'conv3_block4_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block4_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block4_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block1_1_conv (Conv2D)   (None, 14, 14, 256)  131328      ['conv3_block4_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block1_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block1_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_0_conv (Conv2D)   (None, 14, 14, 1024  525312      ['conv3_block4_out[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_0_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
            "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block1_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block1_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block2_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block2_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block2_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block2_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block2_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_out[0][0]',       \n",
            "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block2_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block2_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block3_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block3_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block3_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block3_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block3_add (Add)         (None, 14, 14, 1024  0           ['conv4_block2_out[0][0]',       \n",
            "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block3_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block3_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block4_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block4_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block4_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block4_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block4_add (Add)         (None, 14, 14, 1024  0           ['conv4_block3_out[0][0]',       \n",
            "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block4_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block4_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block5_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block4_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block5_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block5_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block5_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block5_add (Add)         (None, 14, 14, 1024  0           ['conv4_block4_out[0][0]',       \n",
            "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block5_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block5_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block6_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block5_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block6_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block6_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block6_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block6_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block6_add (Add)         (None, 14, 14, 1024  0           ['conv4_block5_out[0][0]',       \n",
            "                                )                                 'conv4_block6_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block6_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block6_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv5_block1_1_conv (Conv2D)   (None, 7, 7, 512)    524800      ['conv4_block6_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block1_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block1_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_0_conv (Conv2D)   (None, 7, 7, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block1_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block1_0_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
            "                                                                  'conv5_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block1_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block2_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block2_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block2_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block2_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_out[0][0]',       \n",
            "                                                                  'conv5_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block2_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block3_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block3_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block3_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block3_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block2_out[0][0]',       \n",
            "                                                                  'conv5_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block3_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23,587,712\n",
            "Trainable params: 23,534,592\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# construct the head of the model that will be placed on top of the\n",
        "# the base model\n",
        "headModel = baseModel.output\n",
        "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(256, activation=\"relu\")(headModel)\n",
        "headModel = Dense(8, activation=\"softmax\")(headModel)\n",
        "# place the head FC model on top of the base model (this will become\n",
        "# the actual model we will train)\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "# loop over all layers in the base model and freeze them so they will\n",
        "# *not* be updated during the training process\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = True"
      ],
      "metadata": {
        "id": "ItfPXKrs1SLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INIT_LR = 0.00001\n",
        "BS = 16\n",
        "NUM_EPOCHS = 80"
      ],
      "metadata": {
        "id": "cyF8JsX-iaxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "opt = Adam(lr=INIT_LR)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "# train the model\n",
        "print(\"[INFO] training model...\")\n",
        "H = model.fit_generator(\n",
        "\ttrainGen,\n",
        "\tvalidation_data=valGen,\n",
        "\tepochs=NUM_EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9usAY0xjiFkL",
        "outputId": "6d49652a-366a-4646-94d6-785e4fe9cefb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training model...\n",
            "Epoch 1/80\n",
            "120/120 [==============================] - 141s 1s/step - loss: 2.2090 - accuracy: 0.1167 - val_loss: 4.1446 - val_accuracy: 0.1000\n",
            "Epoch 2/80\n",
            "120/120 [==============================] - 21s 177ms/step - loss: 1.3645 - accuracy: 0.8687 - val_loss: 15.2018 - val_accuracy: 0.1250\n",
            "Epoch 3/80\n",
            "120/120 [==============================] - 21s 177ms/step - loss: 0.9082 - accuracy: 0.9979 - val_loss: 19.6710 - val_accuracy: 0.1250\n",
            "Epoch 4/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.6065 - accuracy: 1.0000 - val_loss: 22.4243 - val_accuracy: 0.1250\n",
            "Epoch 5/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 0.4192 - accuracy: 1.0000 - val_loss: 21.7071 - val_accuracy: 0.1250\n",
            "Epoch 6/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 0.3023 - accuracy: 1.0000 - val_loss: 16.5292 - val_accuracy: 0.1250\n",
            "Epoch 7/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 0.2280 - accuracy: 1.0000 - val_loss: 9.9756 - val_accuracy: 0.1500\n",
            "Epoch 8/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 0.1776 - accuracy: 1.0000 - val_loss: 4.7596 - val_accuracy: 0.1000\n",
            "Epoch 9/80\n",
            "120/120 [==============================] - 21s 174ms/step - loss: 0.1431 - accuracy: 1.0000 - val_loss: 3.4390 - val_accuracy: 0.1250\n",
            "Epoch 10/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 0.1179 - accuracy: 1.0000 - val_loss: 2.8130 - val_accuracy: 0.0750\n",
            "Epoch 11/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 0.0983 - accuracy: 1.0000 - val_loss: 2.5934 - val_accuracy: 0.0750\n",
            "Epoch 12/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0840 - accuracy: 1.0000 - val_loss: 2.5252 - val_accuracy: 0.0750\n",
            "Epoch 13/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0727 - accuracy: 1.0000 - val_loss: 2.5086 - val_accuracy: 0.0500\n",
            "Epoch 14/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0632 - accuracy: 1.0000 - val_loss: 2.5354 - val_accuracy: 0.0750\n",
            "Epoch 15/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0555 - accuracy: 1.0000 - val_loss: 2.5471 - val_accuracy: 0.0750\n",
            "Epoch 16/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0491 - accuracy: 1.0000 - val_loss: 2.5506 - val_accuracy: 0.1000\n",
            "Epoch 17/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0437 - accuracy: 1.0000 - val_loss: 2.5643 - val_accuracy: 0.1000\n",
            "Epoch 18/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0391 - accuracy: 1.0000 - val_loss: 2.5489 - val_accuracy: 0.1000\n",
            "Epoch 19/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0352 - accuracy: 1.0000 - val_loss: 2.5510 - val_accuracy: 0.1000\n",
            "Epoch 20/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0318 - accuracy: 1.0000 - val_loss: 2.5500 - val_accuracy: 0.1000\n",
            "Epoch 21/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 2.5689 - val_accuracy: 0.0750\n",
            "Epoch 22/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 0.0262 - accuracy: 1.0000 - val_loss: 2.5547 - val_accuracy: 0.1000\n",
            "Epoch 23/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0238 - accuracy: 1.0000 - val_loss: 2.5677 - val_accuracy: 0.0750\n",
            "Epoch 24/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 0.0218 - accuracy: 1.0000 - val_loss: 2.5727 - val_accuracy: 0.0750\n",
            "Epoch 25/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0200 - accuracy: 1.0000 - val_loss: 2.5581 - val_accuracy: 0.0750\n",
            "Epoch 26/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 2.6070 - val_accuracy: 0.1000\n",
            "Epoch 27/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 0.0168 - accuracy: 1.0000 - val_loss: 2.5902 - val_accuracy: 0.0750\n",
            "Epoch 28/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 2.6089 - val_accuracy: 0.0750\n",
            "Epoch 29/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 2.5829 - val_accuracy: 0.0750\n",
            "Epoch 30/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 2.5745 - val_accuracy: 0.0750\n",
            "Epoch 31/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 2.5941 - val_accuracy: 0.1000\n",
            "Epoch 32/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 2.5877 - val_accuracy: 0.1000\n",
            "Epoch 33/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 2.6185 - val_accuracy: 0.0750\n",
            "Epoch 34/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 2.6449 - val_accuracy: 0.1000\n",
            "Epoch 35/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 2.6305 - val_accuracy: 0.0750\n",
            "Epoch 36/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 2.6298 - val_accuracy: 0.0750\n",
            "Epoch 37/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 2.6305 - val_accuracy: 0.0750\n",
            "Epoch 38/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 2.6286 - val_accuracy: 0.0750\n",
            "Epoch 39/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 2.6250 - val_accuracy: 0.0500\n",
            "Epoch 40/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 2.6418 - val_accuracy: 0.0500\n",
            "Epoch 41/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 2.6592 - val_accuracy: 0.0500\n",
            "Epoch 42/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 2.6429 - val_accuracy: 0.1000\n",
            "Epoch 43/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 2.6717 - val_accuracy: 0.1000\n",
            "Epoch 44/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 2.6670 - val_accuracy: 0.0750\n",
            "Epoch 45/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 2.6774 - val_accuracy: 0.0750\n",
            "Epoch 46/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 2.6564 - val_accuracy: 0.0750\n",
            "Epoch 47/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 2.6759 - val_accuracy: 0.0750\n",
            "Epoch 48/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 2.6875 - val_accuracy: 0.0750\n",
            "Epoch 49/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 2.6920 - val_accuracy: 0.0750\n",
            "Epoch 50/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 2.6742 - val_accuracy: 0.0750\n",
            "Epoch 51/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 2.6791 - val_accuracy: 0.0750\n",
            "Epoch 52/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 2.6757 - val_accuracy: 0.0750\n",
            "Epoch 53/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 2.6863 - val_accuracy: 0.0750\n",
            "Epoch 54/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 2.7066 - val_accuracy: 0.0500\n",
            "Epoch 55/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 2.7429 - val_accuracy: 0.0500\n",
            "Epoch 56/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 2.7088 - val_accuracy: 0.0500\n",
            "Epoch 57/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 2.7414 - val_accuracy: 0.0500\n",
            "Epoch 58/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.7230 - val_accuracy: 0.0500\n",
            "Epoch 59/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.7559 - val_accuracy: 0.0750\n",
            "Epoch 60/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.7382 - val_accuracy: 0.0750\n",
            "Epoch 61/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.7466 - val_accuracy: 0.1000\n",
            "Epoch 62/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.7603 - val_accuracy: 0.0750\n",
            "Epoch 63/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.7234 - val_accuracy: 0.0750\n",
            "Epoch 64/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.7590 - val_accuracy: 0.0500\n",
            "Epoch 65/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.7618 - val_accuracy: 0.0500\n",
            "Epoch 66/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.7488 - val_accuracy: 0.0500\n",
            "Epoch 67/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.7472 - val_accuracy: 0.1000\n",
            "Epoch 68/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 9.5276e-04 - accuracy: 1.0000 - val_loss: 2.7771 - val_accuracy: 0.0750\n",
            "Epoch 69/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 8.9207e-04 - accuracy: 1.0000 - val_loss: 2.7714 - val_accuracy: 0.0750\n",
            "Epoch 70/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 8.3426e-04 - accuracy: 1.0000 - val_loss: 2.8365 - val_accuracy: 0.0750\n",
            "Epoch 71/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 7.8231e-04 - accuracy: 1.0000 - val_loss: 2.7900 - val_accuracy: 0.1000\n",
            "Epoch 72/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 7.3172e-04 - accuracy: 1.0000 - val_loss: 2.8106 - val_accuracy: 0.1000\n",
            "Epoch 73/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 6.8563e-04 - accuracy: 1.0000 - val_loss: 2.7831 - val_accuracy: 0.1000\n",
            "Epoch 74/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 6.4240e-04 - accuracy: 1.0000 - val_loss: 2.8146 - val_accuracy: 0.1000\n",
            "Epoch 75/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 6.0013e-04 - accuracy: 1.0000 - val_loss: 2.8047 - val_accuracy: 0.1000\n",
            "Epoch 76/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 5.6227e-04 - accuracy: 1.0000 - val_loss: 2.8199 - val_accuracy: 0.1000\n",
            "Epoch 77/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 5.2732e-04 - accuracy: 1.0000 - val_loss: 2.8239 - val_accuracy: 0.0750\n",
            "Epoch 78/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 4.9426e-04 - accuracy: 1.0000 - val_loss: 2.8196 - val_accuracy: 0.1000\n",
            "Epoch 79/80\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 4.6326e-04 - accuracy: 1.0000 - val_loss: 2.8392 - val_accuracy: 0.1000\n",
            "Epoch 80/80\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 4.3353e-04 - accuracy: 1.0000 - val_loss: 2.8384 - val_accuracy: 0.0750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/My Drive/FEW_SHOT_MULTI_TASK/Comparison/paper2_final_new_keras_retrain.h5\")"
      ],
      "metadata": {
        "id": "kyR5w79kiFlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[INFO] evaluating network...\")\n",
        "testGen.reset()\n",
        "predIdxs = model.predict_generator(testGen)\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "print(confusion_matrix(testGen.classes, predIdxs))\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(testGen.classes, predIdxs,\n",
        "\ttarget_names=testGen.class_indices.keys(), digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOhAu8QPiFqO",
        "outputId": "2628968d-bfa2-4083-bdc5-1ca9b855f247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] evaluating network...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 31  25   8  11   7  12   4   0]\n",
            " [117  36   9  12   5  35  23   1]\n",
            " [ 23  21  10   3   5  28   3   1]\n",
            " [ 55  34  10   5   1  18   7   2]\n",
            " [316 215  86  24   9 130  62  15]\n",
            " [ 44  55   1   8   2  18   5   7]\n",
            " [ 39  52  12  10   4  51  14   6]\n",
            " [ 47  10  26   4   1  34   4   3]]\n",
            "                              precision    recall  f1-score   support\n",
            "\n",
            "             benign_adenosis     0.0461    0.3163    0.0805        98\n",
            "         benign_fibroadenoma     0.0804    0.1513    0.1050       238\n",
            "       benign_phyllodestumor     0.0617    0.1064    0.0781        94\n",
            "       benign_tubularadenoma     0.0649    0.0379    0.0478       132\n",
            "   malignant_ductalcarcinoma     0.2647    0.0105    0.0202       857\n",
            "  malignant_lobularcarcinoma     0.0552    0.1286    0.0773       140\n",
            " malignant_mucinouscarcinoma     0.1148    0.0745    0.0903       188\n",
            "malignant_papillarycarcinoma     0.0857    0.0233    0.0366       129\n",
            "\n",
            "                    accuracy                         0.0672      1876\n",
            "                   macro avg     0.0967    0.1061    0.0670      1876\n",
            "                weighted avg     0.1627    0.0672    0.0514      1876\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# compute the confusion matrix and and use it to derive the raw\n",
        "# accuracy, sensitivity, and specificity\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = H.history['accuracy']\n",
        "val_acc = H.history['val_accuracy']\n",
        "loss = H.history['loss']\n",
        "val_loss = H.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'b:', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.plot(epochs, loss, 'b--', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r-.', label='Validation loss')\n",
        "plt.title('Training Loss and Accuracy on Dataset')\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# === 混淆矩阵：真实值与预测值的对比 ===\n",
        "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "con_mat = confusion_matrix(testGen.classes, predIdxs)\n",
        "\n",
        "con_mat_norm = con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis]     # 归一化\n",
        "\n",
        "con_mat_norm = np.around(con_mat_norm, decimals=4)\n",
        "\n",
        "# === plot ===\n",
        "plt.figure(figsize=(8, 8))\n",
        "sns.heatmap(con_mat_norm, annot=True, cmap='Blues')\n",
        "\n",
        "plt.ylim(0, 8)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.savefig('Confusion matrix',dpi=300, bbox_inches='tight',   pad_inches = 0);\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YUT9EcKJm9rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict_generator(testGen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqj2Ds9Ym9sf",
        "outputId": "4e2b814d-3a23-4136-e887-96deedbfddac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "#roc auc score\n",
        "scr1 = roc_auc_score(testGen.classes, preds, multi_class='ovo', average='macro')\n",
        "scr3 = roc_auc_score(testGen.classes, preds, multi_class='ovr', average='macro')\n",
        "scr2 = roc_auc_score(testGen.classes, preds, multi_class='ovo', average='weighted')\n",
        "scr4 = roc_auc_score(testGen.classes, preds, multi_class='ovr', average='weighted')\n",
        "\n",
        "\n",
        "print(\"macro - ovo ROC AUC Score: \", scr1)\n",
        "print(\"\")\n",
        "\n",
        "print(\"weighted - ovo ROC AUC Score: \", scr2)\n",
        "print(\"\")\n",
        "\n",
        "print(\"macro - ovr ROC AUC Score: \", scr3)\n",
        "print(\"\")\n",
        "\n",
        "print(\"weighted - ovr ROC AUC Score: \", scr4)\n",
        "print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_OpmUR8m9xR",
        "outputId": "a05f4a6a-8414-436f-821d-743cd853c791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "macro - ovo ROC AUC Score:  0.47161400754444716\n",
            "\n",
            "weighted - ovo ROC AUC Score:  0.4767279306269024\n",
            "\n",
            "macro - ovr ROC AUC Score:  0.4726454584590176\n",
            "\n",
            "weighted - ovr ROC AUC Score:  0.48320816033806097\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# roc curve for classes\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "thresh ={}\n",
        "\n",
        "target_names = ['A', 'F', 'PT', 'TA', 'DC','LC', 'MC', 'PC']\n",
        "n_class = 8\n",
        "\n",
        "for i in range(len(target_names)):\n",
        "    fpr[i], tpr[i], thresh[i] = roc_curve(testGen.classes, predIdxs[:], pos_label=i)\n",
        "    auroc = round(metrics.auc(fpr[i], tpr[i]),2)\n",
        "    print('class',i,'-', target_names[i],' :','--AUC--->',auroc)\n",
        "\n",
        "# plotting\n",
        "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0-A: vs Rest')\n",
        "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1- F: vs Rest')\n",
        "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2- PT: vs Rest')\n",
        "plt.plot(fpr[3], tpr[3], linestyle='--',color='yellow', label='Class 3- TA: vs Rest')\n",
        "plt.plot(fpr[4], tpr[4], linestyle='-',color='orange', label='Class 4-DC vs Rest')\n",
        "plt.plot(fpr[5], tpr[5], linestyle='-',color='green', label='Class 5-LC: vs Rest')\n",
        "plt.plot(fpr[6], tpr[6], linestyle='-',color='blue', label='Class 6-MC: vs Rest')\n",
        "plt.plot(fpr[7], tpr[7], linestyle='-',color='yellow', label='Class 7- LC: vs Rest')\n",
        "\n",
        "\n",
        "plt.title('Multiclass ROC-AUC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive rate')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('8 class ROC ',dpi=300, bbox_inches='tight',   pad_inches = 0);\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLH6JyGgnLF9",
        "outputId": "0f285723-ea26-4d60-ba36-55fb75105aef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class 0 - A  : --AUC---> 0.5\n",
            "class 1 - F  : --AUC---> 0.45\n",
            "class 2 - PT  : --AUC---> 0.57\n",
            "class 3 - TA  : --AUC---> 0.45\n",
            "class 4 - DC  : --AUC---> 0.48\n",
            "class 5 - LC  : --AUC---> 0.49\n",
            "class 6 - MC  : --AUC---> 0.6\n",
            "class 7 - PC  : --AUC---> 0.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "acc = sklearn.metrics.accuracy_score(testGen.classes, predIdxs)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3JNT_mqnLG0",
        "outputId": "4a97c2b4-000c-4bfd-f8e8-9acecee59a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.06716417910447761\n"
          ]
        }
      ]
    }
  ]
}